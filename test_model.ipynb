{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_utils import *\n",
    "from model_factory import get_model\n",
    "from dataset_factory import get_datasets\n",
    "\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded vocab\n",
      "Loaded Dataset\n",
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "config_data = read_file_in_dir(sys.path[0], 'default_test' + '.json')\n",
    "vocab, train_loader, val_loader, test_loader = get_datasets(config_data)\n",
    "model = get_model(config_data, vocab)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(os.path.join('experiment_data/baseline_1', 'latest_model.pt'))\n",
    "model.load_state_dict(state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_question(prediction):\n",
    "        \"\"\"\n",
    "        Converts predicted question indices to word tokens\n",
    "        prediction: N x Q\n",
    "        \"\"\"\n",
    "        word_idxs = prediction.cpu().numpy()\n",
    "        captions = []\n",
    "        for i in range(prediction.shape[0]):\n",
    "            words = [vocab.idx2word[idx].lower() for idx in word_idxs[i]]\n",
    "            try:\n",
    "                end_idx = words.index('<end>') + 1 # cut off after predicting end\n",
    "            except ValueError as e:\n",
    "                end_idx = None\n",
    "            \n",
    "            words = words[:end_idx]\n",
    "            captions.append(words)\n",
    "        \n",
    "        to_return = []\n",
    "        for i in range(len(captions)):\n",
    "            clean_list = ['<pad>', '<start>', '<end>', '<unk>', ' ', ';', ',', '.', '\\'', '-', '(', ')', '[', ']', '@', '$', \\\n",
    "                '%', '!', '?', '/', '+', '^', '&', '*']\n",
    "            cleaned_caption = [word for word in captions[i] if word not in clean_list]\n",
    "            to_return.append(cleaned_caption)\n",
    "\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what what the\n",
      "at what time is the harvard yale rivalry set aside\n",
      "what what the the\n",
      "what type of education was assessed during this time\n",
      "what what the the\n",
      "the adaptive immune system must distinguish between what types of molecules\n"
     ]
    }
   ],
   "source": [
    "model.temperature = config_data['generation']['temperature']\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (passages, answers, questions) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available:\n",
    "            passages = passages.cuda().long()\n",
    "            answers = answers.cuda().long()\n",
    "            questions = questions.cuda().long()\n",
    "\n",
    "        # Metric Evaluation\n",
    "        predictions = model.predict(passages, answers) # N x Q\n",
    "        predictions = convert_question(predictions) # list of lists of tokens\n",
    "        true_questions = convert_question(questions) # list of lists of tokens\n",
    "        break\n",
    "\n",
    "print(' '.join(predictions[0]))\n",
    "print(' '.join(true_questions[0]))\n",
    "print(' '.join(predictions[1]))\n",
    "print(' '.join(true_questions[1]))\n",
    "print(' '.join(predictions[2]))\n",
    "print(' '.join(true_questions[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "147ef1215b7e2b4bf3a64983c233460acb54149cdc3836f93d9a84ff7ba2f913"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('mykernel': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
